
import keras
from keras import backend as K
from typing import Union, Callable, Optional
from keras import initializers, activations
from keras.utils import get_custom_objects

class TEDTransformerTransition(keras.layers.Layer):
    """
    Transformer transition function. The same function is used both
    in classical in Universal Transformers. Except that in Universal
    Transformer it is also shared between time steps.
    """

    def __init__(self, 
                 activation: Union[str, Callable] = 'relu',
                 kernel_initializer='glorot_normal',
                 bias_initializer='zeros',
                 kernel_regularizer=None,
                 bias_regularizer=None,
                 kernel_constraint=None,
                 bias_constraint=None,
                 dropout_rate=0.0,
                 size_multiplier: int = 4, **kwargs):
        """
        :param activation: activation function. Must be a string or a callable.
        :param size_multiplier: How big the hidden dimension should be.
          Most of the implementation use transition functions having 4 times
          more hidden units than the model itself.
        :param kwargs: Keras-specific layer arguments.
        """
        self.activation = activations.get(activation)
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.bias_initializer = keras.initializers.get(bias_initializer)
        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = keras.regularizers.get(bias_regularizer)
        self.kernel_constraint = keras.constraints.get(kernel_constraint)
        self.bias_constraint = keras.constraints.get(bias_constraint)
        self.dropout_rate = dropout_rate
        self.size_multiplier = size_multiplier
        super().__init__(**kwargs)

    def get_config(self):
        config = {
            'activation': keras.activations.serialize(self.activation),
            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),
            'bias_initializer': keras.initializers.serialize(self.bias_initializer),
            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),
            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),
            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),
            'bias_constraint': keras.constraints.serialize(self.bias_constraint),
            'dropout_rate': self.dropout_rate,
            'size_multiplier': self.size_multiplier,
            }
        base_config = super(TEDTransformerTransition, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

    def compute_output_shape(self, input_shape):
        return input_shape

    def compute_mask(self, inputs, input_mask=None):
        return input_mask

    # noinspection PyAttributeOutsideInit
    def build(self, input_shape):
        d_model = input_shape[-1]
        self.weights1 = self.add_weight(
            name='{}_weights1'.format(self.name),
            shape=(d_model, self.size_multiplier * d_model),
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            trainable=True)
        self.biases1 = self.add_weight(
            name='{}_biases1'.format(self.name),
            shape=(self.size_multiplier * d_model,),
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            trainable=True)
        self.weights2 = self.add_weight(
            name='{}_weights2'.format(self.name),
            shape=(self.size_multiplier * d_model, d_model),
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            trainable=True)
        self.biases2 = self.add_weight(
            name='{}_biases2'.format(self.name),
            shape=(d_model,),
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            trainable=True)
        return super().build(input_shape)

    def call(self, inputs, mask=None, training=None):
        input_shape = K.shape(inputs)
        d_model = input_shape[-1]
        step1 = self.activation(
            K.bias_add(
                K.dot(K.reshape(inputs, (-1, d_model)),
                      self.weights1),
                self.biases1,
                data_format='channels_last'))
        if 0.0 < self.dropout_rate < 1.0:
            def dropped_inputs():
                return K.dropout(step1, self.dropout_rate, K.shape(step1))
            step1 = K.in_train_phase(dropped_inputs, step1, training=training)
        step2 = K.bias_add(
            K.dot(step1, self.weights2),
            self.biases2,
            data_format='channels_last')
        result = K.reshape(step2, (-1, input_shape[-3], input_shape[-2], input_shape[-1]))
        return result

get_custom_objects().update({
    'TEDTransformerTransition': TEDTransformerTransition,
})

